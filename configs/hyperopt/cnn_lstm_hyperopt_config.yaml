# CNN+LSTM Hyperparameter Optimization Configuration - Task 5.5

# Study Configuration
study:
  name: "cnn_lstm_optimization_task_5_5"
  n_trials: 1000
  timeout: null  # No timeout, run all trials
  directions: ["maximize", "minimize", "minimize"]  # [accuracy, training_time, model_size]
  
# Pruner Configuration
pruner:
  type: "median"  # "median" or "successive_halving"
  n_startup_trials: 20
  n_warmup_steps: 10
  interval_steps: 5
  n_min_trials: 5

# Sampler Configuration  
sampler:
  type: "tpe"  # "tpe", "cmaes", or "random"
  n_startup_trials: 50
  n_ei_candidates: 24
  multivariate: true
  group: true

# Data Configuration
data:
  use_real_data: true
  symbols: ["AAPL", "GOOGL", "MSFT", "TSLA", "NVDA", "AMZN", "META", "NFLX"]
  start_date: "2020-01-01"
  end_date: "2024-01-01"
  timeframes: ["1min", "5min", "15min"]
  sequence_length: 60
  batch_size: 32
  input_dim: 11

# Search Space Configuration
search_space:
  # CNN Architecture
  cnn:
    num_filters: [32, 64, 128, 256]
    filter_sizes: 
      - [3, 5, 7]
      - [3, 5, 7, 11] 
      - [5, 7, 11]
      - [3, 7, 11]
      - [1, 3, 5, 7]
    dropout_rate: [0.1, 0.5]
    use_attention: [true, false]
    attention_heads: [2, 4, 8, 16]
    use_residual: [true, false]
    activation: ["relu", "gelu", "swish"]
  
  # LSTM Architecture
  lstm:
    hidden_dim: [64, 128, 256, 512, 1024]
    num_layers: [1, 4]
    dropout_rate: [0.1, 0.5]
    bidirectional: [true, false]
    use_attention: [true, false]
    attention_heads: [2, 4, 8, 16]
    use_skip_connections: [true, false]
  
  # Feature Fusion
  fusion:
    method: ["concat", "attention", "gated", "bilinear"]
    dim: [128, 256, 512, 1024]
    dropout_rate: [0.1, 0.4]
    num_heads: [4, 8, 16]
  
  # Training Parameters
  training:
    learning_rate: [1e-5, 1e-2]  # log scale
    batch_size: [16, 32, 64, 128]
    weight_decay: [1e-6, 1e-3]  # log scale
    optimizer: ["adam", "adamw", "sgd", "rmsprop"]
    scheduler: ["cosine", "step", "exponential", "plateau"]
    gradient_clip_norm: [0.5, 5.0]
  
  # Regularization
  regularization:
    l1_reg_weight: [0.0, 1e-3]  # log scale
    l2_reg_weight: [1e-6, 1e-3]  # log scale
    dropout_schedule: ["constant", "linear", "cosine"]
    label_smoothing: [0.0, 0.2]
  
  # Multi-task Learning
  multitask:
    classification_weight: [0.3, 0.8]
    regression_weight: [0.2, 0.7]
    task_balancing: ["fixed", "dynamic", "uncertainty"]

# Training Configuration for Trials
trial_training:
  max_epochs: 50
  early_stopping_patience: 10
  validation_frequency: 1
  
# Full Retraining Configuration
retraining:
  enabled: true
  top_k: 5
  full_epochs: 200
  early_stopping_patience: 50
  save_models: true

# Output Configuration
output:
  save_dir: "hyperopt_results"
  log_level: "INFO"
  save_study: true
  save_best_configs: true
  generate_report: true

# MLflow Configuration
mlflow:
  enabled: true
  experiment_name: "CNN_LSTM_Hyperopt_Task_5_5"
  tracking_uri: null  # Use default
  
# Resource Configuration
resources:
  device: null  # Auto-detect
  num_workers: 4
  pin_memory: true