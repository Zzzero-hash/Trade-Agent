# Advanced Optimization Configuration
optimization:
  # Optimizer Settings
  optimizer:
    type: "AdamW"  # Adam, AdamW, SGD, RMSprop
    lr: 1e-4
    weight_decay: 1e-5
    betas: [0.9, 0.999]
    eps: 1e-8
    
  # Learning Rate Scheduling
  lr_scheduler:
    type: "CosineAnnealingWarmRestarts"  # StepLR, ExponentialLR, CosineAnnealingLR
    T_0: 10
    T_mult: 2
    eta_min: 1e-6
    
  # Gradient Management
  gradient:
    clip_norm: 1.0
    clip_value: null
    accumulation_steps: 1
    
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 20
    min_delta: 1e-6
    monitor: "val_loss"
    mode: "min"
    
  # Model Checkpointing
  checkpointing:
    save_top_k: 3
    monitor: "val_sharpe_ratio"
    mode: "max"
    save_last: true
    filename: "epoch_{epoch:02d}-val_sharpe_{val_sharpe_ratio:.4f}"